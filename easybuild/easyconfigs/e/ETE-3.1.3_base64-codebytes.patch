use decodebytes/encodebytes from base64 instead of deprecated variant
author: Karl Johan Westrin (PDC KTH)
diff -ruN ete.orig/ete3/tools/ete_build_lib/db.py ete/ete3/tools/ete_build_lib/db.py
--- ete.orig/ete3/tools/ete_build_lib/db.py     2025-03-03 20:15:03.273035687 +0100
+++ ete/ete3/tools/ete_build_lib/db.py  2025-03-03 20:16:29.436367372 +0100
@@ -69,21 +69,21 @@
         targetconn.commit()
 
 def encode(x):
-    return bytes.decode(base64.encodestring(six.moves.cPickle.dumps(x, 2)))
+    return bytes.decode(base64.encodebytes(six.moves.cPickle.dumps(x, 2)))
 
 def decode(x):
     if six.PY3:
         x = str.encode(x)
         
-    return six.moves.cPickle.loads(base64.decodestring(x))
+    return six.moves.cPickle.loads(base64.decodebytes(x))
 
 # SQLITE_MAX_LENGTH issue: files larger than ~1GB cannot be stored. limit cannot
 # be changed at runtime. Big files are then stored in disk instead
 # def zencode(x):
-#     return base64.encodestring(zlib.compress(cPickle.dumps(x)))
+#     return base64.encodebytes(zlib.compress(cPickle.dumps(x)))
 
 # def zdecode(x):
-#     return cPickle.loads(zlib.decompress(base64.decodestring(x)))
+#     return cPickle.loads(zlib.decompress(base64.decodebytes(x)))
 
 MAX_SQLITE_SIZE = 500000000
 MAX_SQLITE_SIZE = 5
@@ -96,14 +96,14 @@
         six.moves.cPickle.dump(x, open(pjoin(GLOBALS['db_dir'], data_id+".pkl"), "wb"), protocol=1)
         return "__DBDIR__:%s" %data_id
     else:
-        return base64.encodestring(zlib.compress(pdata))
+        return base64.encodebytes(zlib.compress(pdata))
 
 def zdecode(x):
     if x.startswith("__DBDIR__:"):
         data_id = x.split(':', 1)[1]
         data = six.moves.cPickle.load(open(pjoin(GLOBALS['db_dir'], data_id+".pkl"), "rb"))
     else:
-        data = six.moves.cPickle.loads(zlib.decompress(base64.decodestring(x)))
+        data = six.moves.cPickle.loads(zlib.decompress(base64.decodebytes(x)))
     return data
 
 def prevent_sqlite_umask_bug(fname):
